{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ee0a02-5951-4fb4-b3d6-495234eaa321",
   "metadata": {},
   "source": [
    "## Preprocessing a few things before importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56f9a23-aee1-4d1f-827c-2292079e4b3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_3662/2158267922.py:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3662/662357161.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    351\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /tmp/ipykernel_3662/2158267922.py:2 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext=SQLContext(sc)\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "177fe40a-fd2f-4428-ac5a-40aca466b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('pyspark - example toPandas()').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3a7f030-a0ce-4a28-ad8a-85e005c38dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv('vaccination.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8adf282b-b164-4965-a543-b3b62449e4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "164015"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7f9642c-9c01-42b5-9a62-8835bc09b96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iso_code', 'continent', 'location', 'date']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5303a07a-80f7-4417-85d9-be312bf58eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iso_code', 'continent', 'location', 'date']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b046c3c-82c4-4613-b21c-bc5117b6a6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('iso_code', 'string'),\n",
       " ('continent', 'string'),\n",
       " ('location', 'string'),\n",
       " ('date', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a3d1bf3-0efa-4e71-a875-230e7af967d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     date|\n",
      "+---------+\n",
      "|2/24/2020|\n",
      "|2/25/2020|\n",
      "|2/26/2020|\n",
      "|2/27/2020|\n",
      "|2/28/2020|\n",
      "|2/29/2020|\n",
      "| 3/1/2020|\n",
      "| 3/2/2020|\n",
      "| 3/3/2020|\n",
      "| 3/4/2020|\n",
      "| 3/5/2020|\n",
      "| 3/6/2020|\n",
      "| 3/7/2020|\n",
      "| 3/8/2020|\n",
      "| 3/9/2020|\n",
      "|3/10/2020|\n",
      "|3/11/2020|\n",
      "|3/12/2020|\n",
      "|3/13/2020|\n",
      "|3/14/2020|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8760d538-f543-420e-b688-b937f762074d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+---------+\n",
      "|iso_code|continent|   location|     date|\n",
      "+--------+---------+-----------+---------+\n",
      "|     AFG|     Asia|Afghanistan|2/24/2020|\n",
      "|     AFG|     Asia|Afghanistan|2/25/2020|\n",
      "|     AFG|     Asia|Afghanistan|2/26/2020|\n",
      "|     AFG|     Asia|Afghanistan|2/27/2020|\n",
      "|     AFG|     Asia|Afghanistan|2/28/2020|\n",
      "|     AFG|     Asia|Afghanistan|2/29/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/10/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/11/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/12/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/13/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/14/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/15/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/16/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/17/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/18/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/19/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/20/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/21/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/22/2020|\n",
      "|     AFG|     Asia|Afghanistan|3/23/2020|\n",
      "+--------+---------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"date\").like(\"_/__/____\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e6c3ea6-f4da-4c90-9b83-12471a1080c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('date',when(df.date.like(\"_/__/____\"),concat(lit(\"0\"),df.date))\n",
    "          .when(df.date.like(\"_/_/____\"),concat(lit(\"0\"),df.date.substr(1,2),lit(\"0\"),df.date.substr(3,1),df.date.substr(4,5)))\n",
    "          .when(df.date.like(\"__/_/____\"),concat(df.date.substr(1,3),lit(\"0\"),df.date.substr(4,1),df.date.substr(5,5)))\n",
    "          .otherwise(df.date))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "459cec7a-8815-47de-9969-2bc5cc233fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+----------+\n",
      "|iso_code|continent|   location|      date|\n",
      "+--------+---------+-----------+----------+\n",
      "|     AFG|     Asia|Afghanistan|02/24/2020|\n",
      "|     AFG|     Asia|Afghanistan|02/25/2020|\n",
      "|     AFG|     Asia|Afghanistan|02/26/2020|\n",
      "|     AFG|     Asia|Afghanistan|02/27/2020|\n",
      "|     AFG|     Asia|Afghanistan|02/28/2020|\n",
      "|     AFG|     Asia|Afghanistan|02/29/2020|\n",
      "|     AFG|     Asia|Afghanistan|03/01/2020|\n",
      "|     AFG|     Asia|Afghanistan|03/02/2020|\n",
      "|     AFG|     Asia|Afghanistan|03/03/2020|\n",
      "|     AFG|     Asia|Afghanistan|03/04/2020|\n",
      "+--------+---------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b969969d-419d-4f2f-9c71-e159b92209cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df = df.withColumn('date', regexp_replace('date', '/', '-')) \\\n",
    "#.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b19ec3-2525-47d8-903e-7c071c98a107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+----------+\n",
      "|iso_code|continent|   location|      date|\n",
      "+--------+---------+-----------+----------+\n",
      "|     AFG|     Asia|Afghanistan|02-24-2020|\n",
      "|     AFG|     Asia|Afghanistan|02-25-2020|\n",
      "|     AFG|     Asia|Afghanistan|02-26-2020|\n",
      "|     AFG|     Asia|Afghanistan|02-27-2020|\n",
      "|     AFG|     Asia|Afghanistan|02-28-2020|\n",
      "|     AFG|     Asia|Afghanistan|02-29-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-01-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-02-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-03-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-04-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-05-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-06-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-07-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-08-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-09-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-10-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-11-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-12-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-13-2020|\n",
      "|     AFG|     Asia|Afghanistan|03-14-2020|\n",
      "+--------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5955f689-8878-43e9-aa56-ff24d4acaa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn((\"date\"),to_date(col(\"date\"),\"MM-dd-yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8dad4c0-80fd-4c52-b8ca-5d8f86f837da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+----------+\n",
      "|iso_code|continent|   location|      date|\n",
      "+--------+---------+-----------+----------+\n",
      "|     AFG|     Asia|Afghanistan|2020-02-24|\n",
      "|     AFG|     Asia|Afghanistan|2020-02-25|\n",
      "|     AFG|     Asia|Afghanistan|2020-02-26|\n",
      "|     AFG|     Asia|Afghanistan|2020-02-27|\n",
      "|     AFG|     Asia|Afghanistan|2020-02-28|\n",
      "|     AFG|     Asia|Afghanistan|2020-02-29|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-01|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-02|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-03|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-04|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-05|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-06|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-07|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-08|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-09|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-10|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-11|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-12|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-13|\n",
      "|     AFG|     Asia|Afghanistan|2020-03-14|\n",
      "+--------+---------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad099d2e-ab7f-4e28-9dcb-19dc410890c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.toPandas().to_csv(\"clean_covid.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "ebdefb21-506c-44d9-9b1f-3ed62f246079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Since this is a small dataset, there is no need to use spark to export the csv\n",
    "#df.write.csv(\"covid_sql.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
